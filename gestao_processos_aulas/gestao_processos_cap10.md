# Capítulo 10: Inteligência de Processos

---

> Se você não pode medir algo, você não pode entendê-lo. Se você não pode entendê-lo, você não pode controlá-lo. Se você não pode controlá-lo, você não pode melhorá-lo.
>
> *H. James Harrington (1929–)*

---

É uma ideia central do BPM que os processos sejam explicitamente definidos, então executados, e que as informações sobre a execução do processo sejam preparadas e analisadas. Dessa forma, essas informações fornecem um loop de feedback sobre como o processo pode ser redesenhado. Os dados sobre a execução dos processos podem vir de BPMSs nos quais os processos são especificados, mas também de sistemas que não trabalham com um modelo de processo explícito, como sistemas ERP ou sistemas de ticketing. Os dados desses sistemas precisam ser transformados para atender aos requisitos de análise inteligente da execução do processo. Esse campo é tipicamente referido como mineração de processos.

Este capítulo trata do uso inteligente dos dados gerados pela execução do processo. Referimos a esses dados como logs de eventos, cobrindo o que foi feito, quando, por quem e em relação a qual instância do processo. Primeiro, investigamos a estrutura dos logs de eventos, sua relação com os modelos de processo e sua utilidade para monitoramento e controle de processos. Em seguida, discutimos três objetivos principais da análise inteligente de processos, a saber, transparência, desempenho e conformidade. Discutimos a descoberta automática de processos como um passo técnico para alcançar a transparência de como o processo é executado na realidade. Depois, estudamos como a análise de logs de eventos pode fornecer insights sobre o desempenho do processo. Finalmente, discutimos como a conformidade entre logs de eventos e um modelo de processo pode ser verificada.

## 10.1 Execução de Processos e Logs de Eventos

No capítulo anterior, estudamos como um modelo de processo pode ser especificado de forma que um BPMS possa suportar sua execução. Tanto os participantes do processo quanto os proprietários do processo estão envolvidos na execução de processos de negócios. No entanto, suas perspectivas são bastante diferentes. Os participantes do processo trabalham em tarefas, que produzem dados de execução como um subproduto. Chamamos esses dados de logs de eventos. Os proprietários do processo estão particularmente interessados em tirar conclusões a partir desses logs de eventos. Nesta seção, discutimos quais perguntas podem ser respondidas usando dados de eventos e como os logs de eventos e os modelos de processo se relacionam entre si.

### 10.1.1 A Perspectiva dos Participantes na Execução do Processo

Quando um processo é executado em um BPMS ou outro software, há uma clara separação entre a coordenação e a execução das tarefas. O sistema geralmente cuida da coordenação de casos individuais, informando os participantes sobre quais tarefas eles precisam trabalhar. Assim, os participantes geralmente veem apenas aquelas tarefas pelas quais são diretamente responsáveis, enquanto o sistema oculta a complexidade do processo como um todo. Cada participante normalmente tem uma lista de trabalho pessoal que mostra o conjunto de itens de trabalho que ainda precisam ser trabalhados. Se um modelo de processo explícito existir, cada um desses itens de trabalho corresponde a uma tarefa no modelo de processo. No entanto, pode haver vários itens de trabalho correspondentes a uma única tarefa se vários casos estiverem sendo trabalhados simultaneamente. Por exemplo, em um momento específico, Chuck, como participante do processo, pode ver que quatro itens de trabalho estão em sua lista de trabalho, todos relacionados à tarefa "Confirmar pedido" do processo de atendimento de pedidos: um item de trabalho relacionado a um pedido do cliente A, um do cliente B e dois do cliente C.

A estrutura de um item de trabalho é definida no modelo de processo executável ou diretamente implementada no software. Isso significa que os participantes veem os campos de dados que foram declarados como entrada para uma tarefa. Para cada item de trabalho em que estão trabalhando, eles devem documentar pelo menos a conclusão. Dessa forma, o sistema pode acompanhar o estado do processo em qualquer momento. Entre outras coisas, é fácil registrar em que ponto no tempo alguém começou a trabalhar em um item de trabalho, quais dados de entrada estavam disponíveis, quais dados de saída foram criados e quem foi o participante que trabalhou nele. Por exemplo, quando Chuck confirma o pedido do cliente B, ele insere o resultado no sistema, e o sistema pode decidir automaticamente se a próxima etapa deve ser emitir a fatura ou escalar a confirmação do pedido para alguém acima de Chuck. A maioria dos BPMSs e também outros sistemas de informação registram esses dados sobre o que foi feito em que momento. O arquivo no qual esses dados são armazenados é chamado de arquivo de log, e os dados nele são chamados de logs de eventos. Cada vez que outra tarefa é concluída, uma nova entrada é adicionada ao arquivo de log. Ou seja, uma vez que Chuck insere seus dados, o sistema adiciona uma linha no arquivo de log indicando que Chuck confirmou um pedido com um carimbo de data e hora correspondente.

### 10.1.2 A Perspectiva do Proprietário do Processo na Execução do Processo

Os logs de eventos têm o potencial de revelar insights importantes sobre como um processo funciona na realidade. Portanto, o proprietário do processo está mais interessado em analisá-los de forma sistemática. Em essência, distinguimos três cenários principais de aplicação para o uso de logs de eventos: descoberta automática de processos, análise de desempenho e verificação de conformidade, todos relacionados a perguntas que o proprietário do processo pode fazer.

**Qual é o modelo de processo real?** A descoberta automática de processos está preocupada com a questão de como um processo realmente funciona na realidade. No Capítulo 5, mencionamos que os logs de eventos podem ser usados como entrada para a descoberta de processos baseada em evidências. A descoberta automática de processos utiliza logs de eventos para a geração de um modelo de processo correspondente. Dessa forma, os logs de eventos são valiosos para encontrar um modelo de processo onde antes não existia e para ajustar um modelo existente de acordo com como o processo realmente funciona.

**Qual é o desempenho do processo?** No Capítulo 7, discutimos que as análises de processo, como a análise de fluxo, sofrem do fato de que o tempo médio de ciclo para cada tarefa no modelo de processo precisa ser estimado. Além disso, muitas vezes são necessárias suposições fortes de que o comportamento do processo não é influenciado pela carga. Usando logs de eventos, podemos inspecionar o comportamento real de um processo e compará-lo com insights da análise de processo. Além disso, informações históricas sobre a execução do processo podem ser aproveitadas para tomar decisões operacionais.

**Em que medida as regras do modelo de processo são seguidas?** A verificação de conformidade é um conjunto de técnicas que compara um conjunto de logs de eventos com um conjunto de restrições ou um modelo de processo existente. Existem situações em que os modelos de processo são definidos, mas não são estritamente aplicados por um BPMS correspondente. Nessas situações, a verificação de conformidade pode ser utilizada para determinar com que frequência o processo é executado conforme esperado e, se não for, em quais etapas as divergências podem ser encontradas. Aqui, os logs de eventos ajudam a entender onde o modelo precisa ser corrigido ou onde o comportamento dos participantes que trabalham no processo precisa ser adaptado.

Fornecendo respostas a esses três tipos de perguntas, podemos obter insights sobre o processo, o que pode ajudar a reposicioná-lo no Quadrilátero do Diabo. Especificamente, a dimensão do tempo aumenta em transparência ao investigar os logs de eventos: os carimbos de data e hora mostram quando as tarefas são executadas e quanto tempo elas levam. Também há uma forte associação com custo se o tempo de trabalho dos participantes no processo puder ser atribuído a uma instância de processo específica. A flexibilidade pode ser analisada com base nos diferentes caminhos que um processo segue. O conjunto histórico de caminhos realmente usados e sua variedade dão uma indicação para essa dimensão. Finalmente, questões de qualidade também podem ser identificadas a partir dos logs de eventos, por exemplo, ao inspecionar o número de retrabalhos e iterações necessárias para uma tarefa específica.

O proprietário do processo pode usar logs de eventos como entrada para dois mecanismos de controle diferentes: em um nível agregado e em um nível de instância. Os mecanismos são chamados de controle de processo e monitoramento de processo, respectivamente.

**Controle de Processo** lida com a análise da execução histórica do processo. A entrada para o controle de processo são logs de eventos que se referem a um determinado período, por exemplo, um trimestre ou um ano inteiro. O controle de processo fornece insights sobre se os objetivos gerais de um processo foram alcançados e se os KPIs estão alinhados. Tipicamente, o controle de processo é uma atividade offline, que envolve logs de execuções de processos concluídas.

**Monitoramento de Processo** está preocupado com a qualidade das instâncias de processo atualmente em execução. A entrada para o monitoramento de processo são os logs de eventos de instâncias de processos individuais ou casos. O monitoramento de processo trabalha com objetivos e regras que são formulados para esses casos individuais e aciona contra-ações quando essas regras são violadas, por exemplo, quando uma solicitação de cliente não é respondida a tempo. Tipicamente, o monitoramento de processo é uma atividade contínua online que envolve eventos de instâncias atualmente em execução.

Tanto o monitoramento de processo quanto o controle de processo desempenham um papel importante no alinhamento do processo com seus objetivos de negócios gerais. Nesse sentido, estão intimamente relacionados às ideias de gestão da qualidade e ao ciclo Plan-do-check-act (PDCA). O PDCA pode ser considerado uma inspiração para o conceito de um ciclo de vida de gestão de processos de negócios, conforme discutido no primeiro capítulo deste livro. O monitoramento e controle de processos (verificação) investigam os dados da execução dos processos (fazer) para que medidas de redesenho (agir) possam ser tomadas para realinhar a execução com os objetivos (planejar). Todos esses conceitos inspiraram a ideia de um cockpit de processo como uma ferramenta de software onde dados sobre a execução de processos são fornecidos online usando gráficos e visualização apropriada.

## 10.1.3 Estrutura dos Logs de Eventos

O monitoramento e controle de processos dependem fortemente dos dados de eventos registrados durante a execução dos processos. Os logs de eventos contêm um conjunto de eventos. Assim, podemos entender um log de eventos como uma lista de registros de eventos. A Figura 10.1 dá uma ilustração de quais dados são tipicamente armazenados nos logs de eventos. Podemos ver que um único evento tem um ID de evento único. Além disso, refere-se a um caso individual, tem um carimbo de data e hora e mostra quais recursos executaram qual tarefa. Esses podem ser participantes (por exemplo, Chuck e Susi) ou sistemas de software (SYS1, SYS2, DMS). Para várias técnicas de análise que discutiremos neste capítulo, é um requisito mínimo que os eventos no log se refiram a (i) um caso, (ii) uma tarefa e (iii) um ponto no tempo. Tendo essas três informações disponíveis, podemos, por exemplo, descobrir um modelo de processo a partir dos logs. Na prática, muitas vezes há informações adicionais armazenadas para cada evento, como custos, sistema utilizado ou dados sobre o caso de negócios tratado. Esses podem ser usados para agrupamento, correlação ou para encontrar relações causais nos logs de eventos.

O log de eventos da Figura 10.1 é capturado como uma lista em formato tabular. O problema com os logs de eventos é que cada fornecedor e sistema de software define formatos de log individuais. Para aproveitar a adoção de ferramentas de análise de logs de eventos, como a ferramenta de código aberto ProM, o IEEE Task Force on Process Mining promove o uso do formato eXtensible Event Stream (XES). Várias ferramentas trabalham com logs de eventos XES ou oferecem recursos para converter logs de eventos para este formato. O metamodelo do XES é ilustrado na Figura 10.2. Cada arquivo XES representa um log. Ele contém múltiplas trilhas, e cada trilha pode conter múltiplos eventos. Todos eles podem conter diferentes atributos. Um atributo deve ser um elemento de string, data, int, float ou booleano como um par chave-valor. Os atributos devem se referir a uma definição global. Existem dois elementos globais no arquivo XES, um para definir atributos de trilha, o outro para definir atributos de evento. Vários classificadores podem ser definidos no XES. Um classificador mapeia um ou mais atributos de um evento para um rótulo que é usado na saída da ferramenta de análise. Dessa forma, por exemplo, eventos podem ser associados a atividades.

A Figura 10.3 mostra como partes das informações do exemplo de log de eventos da Figura 10.1 podem ser armazenadas em um arquivo XES. A partir do primeiro elemento "global" (escopo = "trilha"), vemos que cada elemento de trilha deve ter um atributo "concept:name". Para a trilha definida abaixo, este atributo tem o valor 1. Além disso, existem três atributos diferentes esperados para um evento ("elemento global com escopo = "evento"): "concept:name", "time:timestamp" e "resource". Na trilha definida abaixo, observamos dois eventos. O primeiro refere-se a "Verificar disponibilidade de estoque", que foi concluído pelo SYS1 em 30 de julho de 2012 às 11:14. O segundo evento captura "Recuperar produto do armazém" realizado por Rick às 14:20.

## 10.1.4 Desafios da Extração de Logs de Eventos

Os dados de eventos que estão disponíveis em algum formato tabular, como visualizado na Figura 10.1, podem ser prontamente convertidos para XES e, em seguida, analisados usando ferramentas apropriadas. Em muitos casos, porém, os dados relevantes para logs de eventos não estão diretamente acessíveis no formato necessário, mas precisam ser extraídos de diferentes fontes e integrados. Portanto, podemos identificar cinco grandes desafios para a extração de dados de log, potencialmente exigindo um esforço considerável em um projeto. Esses desafios são:

1. **Desafio de Correlação**: Refere-se ao problema de identificar a qual caso um evento pertence. Muitos sistemas de informação não têm uma noção explícita de processo definida. Portanto, precisamos investigar qual atributo das entidades relacionadas ao processo pode servir como um identificador de caso. Muitas vezes, é possível utilizar identificadores de entidades, como número de pedido, número de fatura ou número de remessa.

2. **Desafio de Timestamps**: O desafio de trabalhar corretamente com timestamps decorre do fato de que muitos sistemas de informação não consideram o registro como uma tarefa primária. Isso significa que o registro é frequentemente atrasado até que o sistema tenha tempo ocioso ou pouca carga. Portanto, podemos encontrar eventos sequenciais com o mesmo timestamp no log. Esse problema é agravado quando logs de diferentes sistemas de informação, potencialmente operando em diferentes fusos horários, precisam ser integrados. Parcialmente, esses problemas podem ser resolvidos com conhecimento de domínio, por exemplo, quando se sabe que os eventos sempre ocorrem em uma ordem específica.

3. **Desafio de Snapshots**: Este ponto refere-se à questão de ter dados de log disponíveis por um certo período de tempo. Para processos de longa duração, pode não ser possível observar todos os casos do log com sua duração completa de ponta a ponta no período considerado. É uma boa ideia excluir tais casos incompletos com cabeça ou cauda faltando. No entanto, deve-se estar ciente de que tal filtragem também pode introduzir um viés, por exemplo, considerando apenas casos breves. Portanto, o período de tempo refletido pelo log deve ser significativamente mais longo do que a duração média de um caso.

4. **Desafio de Escopo**: O escopo do espectro de eventos é um desafio quando o sistema de informação disponível não produz diretamente logs de eventos. Sistemas de informação, como sistemas ERP, registram uma quantidade extensa de eventos relacionados ao processo em inúmeras tabelas. Os logs de eventos devem ser gerados a partir das entradas nessas tabelas, o que requer um entendimento detalhado da semântica dos dados. Tal expertise no sistema pode não estar prontamente disponível.

5. **Desafio de Granularidade**: Tipicamente, estamos interessados em conduzir a análise de logs de eventos em um nível conceitual para o qual temos modelos de processo definidos. Em geral, a granularidade do registro de logs de eventos é muito mais fina, de forma que cada atividade de um modelo de processo pode mapear para um conjunto de eventos. Por exemplo, uma atividade como "Recuperar produto do armazém" no nível de abstração de um modelo de processo mapeia para uma série de eventos como "Item de trabalho #1.211 atribuído", "Item de trabalho #1.211 iniciado", "Formulário de pedido aberto", "Produto recuperado" e "Item de trabalho #1.211 concluído". Muitas vezes, eventos de granularidade fina podem aparecer repetidamente nos logs, enquanto em um nível abstrato apenas uma única tarefa é executada. Portanto, é difícil definir um mapeamento preciso entre os dois níveis de abstração.

## 10.2 Descoberta Automática de Processos

A descoberta automática de processos é uma técnica específica de mineração de processos. O objetivo da descoberta automática de processos é construir um modelo de processo que capture o comportamento de um log de eventos de maneira representativa. A construção deve funcionar automaticamente e de forma genérica, usando um algoritmo que deve fazer suposições mínimas sobre as propriedades do log e do modelo de processo resultante. Ser representativo neste contexto significa que o modelo de processo construído deve ser capaz de reproduzir os casos do log de eventos e proibir comportamentos não encontrados nos logs. A seguir, discutiremos as suposições de dados de log, apresentaremos o algoritmo α como um algoritmo básico de descoberta e abordaremos a noção de representatividade com mais detalhes.

### 10.2.1 Suposições do Algoritmo α

O algoritmo α é um algoritmo básico para descobrir modelos de processos a partir de logs de eventos. É básico pois é menos complexo do que outros algoritmos mais avançados. Além disso, faz certas suposições sobre os logs de eventos fornecidos que discutiremos posteriormente como sendo parcialmente problemáticas. Essas suposições são as seguintes:

- **Ordem dos Eventos**: Os eventos no log estão ordenados cronologicamente. Tal ordem cronológica pode ser definida com base em timestamps.
- **Referência ao Caso**: Cada evento se refere a um único caso.
- **Referência à Atividade**: Cada evento está relacionado a uma atividade específica do processo.
- **Completude da Atividade**: Cada atividade do processo está incluída no log.
- **Completude Comportamental**: O log é comportamentalmente completo no sentido de que se uma atividade a pode ser seguida diretamente por uma atividade b, então há pelo menos um caso no log onde observamos ab.

As três primeiras suposições referem-se ao conteúdo informacional de um evento nos logs. Essas suposições são bastante gerais e não restritivas. O critério de completude da atividade refere-se ao fato de que podemos incluir apenas aquelas atividades no modelo de processo gerado que observamos nos logs. A completude comportamental tem as implicações mais fortes. Na prática, dificilmente podemos assumir que encontramos o conjunto completo de opções comportamentais em um log. Técnicas avançadas tentam fazer suposições mais fracas nesse ponto.

De acordo com essas suposições, usamos um chamado log de workflow como ponto de partida para usar o algoritmo α. A Figura 10.4 mostra como um log de workflow pode ser construído a partir de um log de eventos. A seguir, trabalharemos com letras como referência às tarefas. Um log de workflow é uma coleção de todas as sequências de execução únicas observadas no log. O algoritmo α não distingue com que frequência uma sequência de execução específica foi observada em um log de workflow.

### 10.2.2 As Relações de Ordem do Algoritmo α

O algoritmo α funciona em duas fases para construir um modelo de processo. Na primeira fase, diferentes relações de ordem são extraídas do log de workflow L. Na segunda fase, o modelo de processo é construído de forma incremental a partir dessas relações identificadas. As relações de ordem referem-se a tarefas que seguem diretamente uma à outra no log. Elas fornecem a base para a definição de três relações mais específicas que se referem à causalidade, ao potencial de paralelismo e à não-sucesão. Referimo-nos a este conjunto de relações como as relações α.

- **A relação básica de ordem a>b** é válida se podemos observar no log de workflow L que uma tarefa a é seguida diretamente por b.
- **A relação de causalidade a → b** é derivada da relação básica. É válida se observamos em L que a>b e que b  não>a.
- **A relação de potencial de paralelismo a  b** é válida se tanto a>b quanto b>a são observadas no log de workflow L.
- **A relação de não-sucesão direta a#b** é válida se a ≯ b e b ≯ a.

A razão pela qual exatamente essas relações são usadas é mostrada na Figura 10.5. Existem cinco combinações características de relações entre as tarefas em um log de workflow que podem ser mapeadas para padrões simples de fluxo de controle.

- **Padrão (a)** representa uma sequência de tarefas a e b. Se as modelarmos dessa forma, deve ser garantido que no log de workflow encontraremos a seguido por b, ou seja, a>b, mas nunca b seguido por a, ou seja, b  > a. Isso significa que a relação de causalidade a → b deve ser válida.
- **Padrão (b)** também se relaciona a uma combinação característica de relações. O log de workflow deve mostrar que a → b e a → c são válidas, e que b e c não seriam sucessores mútuos, ou seja, b#c.
- **Padrão (c)** também exige que b e c não sejam sucessores mútuos, ou seja, b#c, enquanto tanto b → d quanto c → d devem ser válidos.
- **Padrão (d)** exige que a → b e a → c sejam válidas, e que b e c mostrem potencial de paralelismo, ou seja, b  c.
- **Padrão (e)** refere-se a b → d e c → d enquanto b e c mostram potencial de paralelismo, ou seja, b  c.

A ideia do algoritmo α é identificar as relações entre todos os pares de tarefas do log de workflow para reconstruir um modelo de processo com base nos Padrões (a) a (e). Portanto, antes de aplicar o algoritmo α, primeiro precisamos extrair todas as relações básicas de ordem do log de workflow L. Considere o log de workflow representado na Figura 10.4 contendo os dois casos a,b,g,h,j,k,i,l e a,c,d,e,f,g,j,h,i,k,l. Desse log de workflow, podemos derivar as seguintes relações.

- **As relações básicas de ordem >** referem-se a cada par de tarefas que seguem diretamente uma à outra. Pode ser lido diretamente do log:
  - a>b h>j i>l d>e g>j i>k
  - b>g j>k a>c e>f j>h k>l
  - g>h k>i c>d f >g h>i
- **As relações causais** podem ser encontradas quando verificamos se cada relação de ordem não é válida na direção oposta. Isso é válido para todos os pares, exceto (h,j) e (i,k), respectivamente. Obtemos:
  - a → b j → k a → c d → e f → g h → i
  - b → g i → l c → d e → f g → j k → l
  - g → h
- **A relação de potencial de paralelismo** é válida para h  j, bem como para k  i (e os casos simétricos correspondentes).
- **A relação restante de não-sucesão direta** pode ser encontrada para todos os pares que não pertencem a → e . Pode ser facilmente derivada quando escrevemos as relações em uma matriz, como mostrado na Figura 10.6. Essa matriz também é referida como a matriz de pegadas do log.

### 10.2.3 O Algoritmo α

O algoritmo α é um algoritmo básico para descoberta automática de processos que toma um log de eventos L e suas relações α como ponto de partida. A ideia essencial do algoritmo é que tarefas que seguem diretamente uma à outra no log devem estar diretamente conectadas no modelo de processo. Além disso, se houver mais de uma tarefa que pode seguir após outra, precisamos determinar se o conjunto de tarefas subsequentes é parcialmente exclusivo ou concorrente. Uma exceção ao princípio de que as tarefas devem estar conectadas no modelo de processo são aquelas que são potencialmente paralelas, ou seja, aqueles pares incluídos em . Os detalhes do algoritmo α são definidos de acordo com os seguintes oito passos:

1. Identificar o conjunto de todas as tarefas no log como TL.
2. Identificar o conjunto de todas as tarefas que foram observadas como a primeira tarefa em algum caso como TI.
3. Identificar o conjunto de todas as tarefas que foram observadas como a última tarefa em algum caso como TO.
4. Identificar o conjunto de todas as conexões a serem potencialmente representadas no modelo de processo como um conjunto XL. Adicionar os seguintes elementos a XL:
   - Padrão (a): todos os pares para os quais a → b é válida.
   - Padrão (b): todos os triplos para os quais a → (b#c) é válido.
   - Padrão (c): todos os triplos para os quais (b#c) → d é válido.
   - Note que triplos para os quais o Padrão (d) a → (b  c) ou o Padrão (e) (b  c) → d são válidos não são incluídos em XL.
5. Construir o conjunto YL como um subconjunto de XL por:
   - Eliminar a → b e a → c se existir a → (b#c).
   - Eliminar b → c e b → d se existir (b#c) → d.
6. Conectar eventos de início e fim da seguinte maneira:
   - Se houver várias tarefas no conjunto TI de primeiras tarefas, então desenhe um evento de início levando a uma divisão (XOR ou AND dependendo da relação entre as tarefas), que se conecta a cada tarefa em TI. Caso contrário, conecte diretamente o evento de início com a única primeira tarefa.
   - Para cada tarefa no conjunto TO de últimas tarefas, adicione um evento de fim e desenhe um arco da tarefa para o evento de fim.
7. Construir os arcos de fluxo da seguinte maneira:
   - Padrão (a): Para cada a → b em YL, desenhe um arco de a para b.
   - Padrão (b): Para cada a → (b#c) em YL, desenhe um arco de a para uma divisão XOR, e de lá para b e c.
   - Padrão (c): Para cada (b#c) → d em YL, desenhe um arco de b e c para uma junção XOR, e de lá para d.
   - Padrão (d) e (e): Se uma tarefa no modelo de processo construído tem múltiplos arcos de entrada ou múltiplos arcos de saída, agrupe esses arcos com uma divisão AND ou junção AND, respectivamente.
8. Retorne o modelo de processo recém-construído.

Vamos passar pelo algoritmo α com o log de workflow L = [a,b,g,h,j,k,i,l,a,c,d,e,f,g,j,h,i,k,l] como um exemplo de entrada. Os Passos 1–3 identificam TL = {a,b,c,d,e,f,g,h,i,j,k,l}, TI = {a} e TO = {l}. No Passo 4a, todas as relações causais são adicionadas a XL, incluindo a → b e a → c, etc. No Passo 4b, trabalhamos linha por linha através da matriz de pegadas da Figura 10.6 e verificamos se há células compartilhando uma relação a → enquanto se relacionam a tarefas que são pareadas em #. Na linha a, observamos tanto a → b quanto a → c. Além disso, b#c é válido. Portanto, adicionamos a → (b#c) a XL. Também consideramos a linha g e sua relação com h e j. No entanto, como h  j é válido, não os adicionamos. No Passo 4c, progredimos coluna por coluna através da matriz de pegadas e verificamos se há células compartilhando uma relação a → enquanto se relacionam a tarefas que são mutuamente em #. Na coluna g, observamos duas relações → para b e f. Além disso, b#f é válido. Portanto, adicionamos (b#f) → g a XL. Também verificamos i e k que compartilham a mesma relação com l. No entanto, como i  k é válido, não os adicionamos. Não há mais combinações complexas encontradas no Passo 4d.

No Passo 5, eliminamos os elementos básicos em XL que são cobertos pelos padrões complexos encontrados nos Passos 4b e 4c. Consequentemente, excluímos a → b, a → c, b → g e f → g. No Passo 6a, introduzimos um evento de início e o conectamos a a; em 6b, a tarefa l é conectada a um evento de fim. No Passo 7, arcos e gateways são adicionados para os elementos de YL. Finalmente, no Passo 8, o modelo de processo é retornado. O modelo resultante é mostrado na Figura 10.7.

### 10.2.4 Descoberta Robusta de Processos

Claramente, o algoritmo α tem seus méritos. Ele pode reconstruir um modelo de processo a partir de um log de eventos comportamentalmente completo, se esse log tiver sido gerado a partir de um modelo de processo estruturado. No entanto, também há limitações a serem observadas. O algoritmo α não é capaz de distinguir os chamados loops curtos do verdadeiro paralelismo. Como pode ser visto na Figura 10.8, todos os três modelos podem produzir os logs de workflow que resultam em b  c na pegada correspondente. Várias extensões ao algoritmo α foram propostas. A ideia do algoritmo α+ é definir a relação  de maneira mais restrita, de forma que b  c seja incluída apenas se não houver uma sequência bcb nos logs. Dessa forma, os modelos (a) e (b) na Figura 10.8 podem ser distinguidos um do outro em seus logs gerados. Além disso, podemos usar pré-processamento para extrair repetições diretas como aa ou bb dos logs, anotar as tarefas correspondentes e continuar com um log do qual esse comportamento repetido é mapeado para uma única execução.

Outros problemas para o algoritmo α são incompletude e ruído. A noção de completude assumida pelo algoritmo α refere-se à relação > da qual as outras relações são derivadas. O número de casos diferentes necessários aumenta com o fatorial do número de tarefas potencialmente concorrentes. Frequentemente, esse número é baixo. Mas já para 10 tarefas concorrentes, precisamos de 10! = 3.628.800 casos. Portanto, é desejável usar algoritmos que possam distinguir explicitamente comportamentos prováveis e improváveis para generalizar quando os logs não são completos. Essa direção também ajuda a abordar problemas com ruído. Os logs de eventos frequentemente incluem casos com uma cabeça ausente, uma cauda ausente ou um episódio intermediário ausente. Além disso, pode haver erros de registro com eventos sendo trocados ou registrados duas vezes. Esse comportamento improvável não deve distorcer o resultado da descoberta do processo.

Várias abordagens foram definidas para abordar problemas de completude e ruído. Em geral, elas tentam equilibrar quatro critérios de qualidade essenciais, a saber, adequação, simplicidade, precisão e generalização. Adequação refere-se ao grau de comportamento do log que um modelo de processo é capaz de reproduzir. Pode ser definida com base na fração de padrões de eventos representados pelo modelo ou com base na fração de casos que podem ser reproduzidos no modelo. Simplicidade significa que o modelo de processo resultante deve ser facilmente compreensível. Pode ser medida usando diferentes métricas de complexidade para modelos de processo, como tamanho do modelo ou grau de estruturação. Precisão refere-se ao grau de comportamento que é permitido pelo modelo, mas não observado nos logs. Podemos facilmente criar um modelo de processo que permite a execução de todas as tarefas em qualquer ordem arbitrária com repetição potencial. No entanto, dificilmente podemos aprender qualquer especificidade do processo a partir de tal modelo. Generalização refere-se à capacidade de um modelo de processo de abstrair do comportamento documentado nos logs. Uma técnica de descoberta que é capaz de generalizar ajuda a trabalhar com comportamento incompleto.

## 10.3 Análise de Desempenho

Na Seção 7.1, introduzimos as quatro dimensões de desempenho de tempo, custo, qualidade e flexibilidade. Por sua vez, o Capítulo 8 demonstrou que essas quatro dimensões formam um Quadrilátero do Diabo quando tentamos melhorar um processo. Essas medidas de desempenho são geralmente consideradas como geralmente relevantes para qualquer tipo de negócio. Além desse conjunto geral, uma empresa deve identificar medidas específicas. Frequentemente, as medidas são específicas da indústria, como lucro por metro quadrado na gastronomia, taxa de retorno no comércio eletrônico ou rotatividade de clientes no marketing. Qualquer medida específica que uma empresa pretenda definir deve ser precisa, econômica e fácil de entender. Aqui, focamos nas quatro medidas gerais de desempenho de tempo, custo, qualidade e flexibilidade. A questão desta seção é como podemos detectar que um processo não está funcionando bem de acordo com uma dessas dimensões. Os logs de eventos nos fornecem dados muito detalhados que são relevantes para o desempenho. Descreveremos técnicas que nos ajudam a medir e visualizar potenciais problemas de desempenho relacionados ao tempo, custo, qualidade e flexibilidade.

### 10.3.1 Medição de Tempo

Tempo e suas medidas mais específicas de tempo de ciclo e tempo de espera são importantes medidas gerais de desempenho. Os logs de eventos geralmente mostram timestamps de forma que possam ser usados para análise de tempo. A análise de tempo está preocupada com a ocorrência temporal e probabilidades de diferentes tipos de eventos. Os logs de eventos de um processo geralmente relacionam cada evento ao ponto no tempo de sua ocorrência. Portanto, é direto plotar eventos no eixo do tempo. Além disso, podemos utilizar classificadores para agrupar eventos em um segundo eixo. Um classificador geralmente se refere a um dos atributos de um evento, como ID do caso ou ID do participante. Existem dois níveis de detalhe para plotar eventos em um diagrama: gráficos pontilhados usando o timestamp para plotar um evento e gráfico de linha do tempo mostrando a duração de uma tarefa e seu tempo de espera.

O gráfico pontilhado é uma ferramenta de visualização simples, mas poderosa para logs de eventos. Cada evento é plotado em uma tela bidimensional com o primeiro eixo representando sua ocorrência no tempo e o segundo eixo como sua associação com um classificador, como um ID de caso. Existem diferentes opções para organizar o primeiro eixo. O tempo pode ser representado de forma relativa, de forma que o primeiro evento seja contado como zero, ou absoluta, de forma que casos posteriores com um evento inicial posterior estejam mais à direita em comparação com casos que começaram anteriormente. O segundo eixo pode ser ordenado de acordo com diferentes critérios. Por exemplo, os casos podem ser mostrados de acordo com sua ordem histórica ou seu tempo de ciclo total. A Figura 10.9 mostra o gráfico pontilhado dos logs de um processo de saúde. Os eventos são plotados de acordo com seu tempo relativo e ordenados de acordo com seu tempo de ciclo total. Pode-se ver que há uma variação considerável em termos de tempo de ciclo. Além disso, o gráfico sugere que pode haver três classes distintas de casos: aqueles que levam no máximo 60 dias, aqueles que levam 60 a 210 dias e uma pequena classe de casos que levam mais de 210 dias. Tal inspeção exploratória pode fornecer uma boa base para uma análise mais detalhada dos fatores que influenciam o tempo de ciclo.

A análise temporal dos logs de eventos pode ser aprimorada com mais detalhes se um modelo de processo correspondente estiver disponível e as tarefas puderem ser relacionadas a um evento de início e término. A ideia é utilizar o conceito de reprodução de token para identificar o ponto no tempo em que uma tarefa é ativada. Para tarefas em uma sequência, o tempo de ativação é o ponto no tempo em que a tarefa anterior foi concluída. Para tarefas após uma junção AND, este é o ponto no tempo em que todas as tarefas anteriores foram concluídas. Para junções e divisões XOR, é o ponto quando uma das tarefas anteriores é concluída.

Usando essas informações, podemos plotar uma tarefa não como um ponto, mas como uma barra em um gráfico de linha do tempo (veja a Figura 10.10). Um gráfico de linha do tempo mostra um tempo de espera (da ativação até o início) e um tempo de processamento (do início até a conclusão) para cada tarefa. As linhas do tempo de cada tarefa podem ser visualizadas de forma semelhante a um ponto no gráfico pontilhado. O gráfico de linha do tempo é mais informativo do que o gráfico pontilhado, pois mostra a duração das tarefas. Além disso, é útil ver os tempos de espera. Ambas as informações são uma entrada valiosa para a análise quantitativa de processos. Quando milhares de casos estão disponíveis como um log, pode-se estimar a distribuição do tempo de espera e do tempo de processamento de cada tarefa. Dessa forma, gargalos com longos tempos de espera podem ser identificados e, da mesma forma, tarefas que são mais promissoras para focar esforços de redesenho. Além disso, essas informações também podem ser usadas para prever os tempos de execução de instâncias de processos em execução, o que é útil para o monitoramento de processos.

### 10.3.2 Medição de Custos

Em um contexto de processo, a medição de custos está principalmente relacionada ao problema de atribuir custos indiretos aos casos. Afinal, custos diretos como os custos de compra de quatro rodas que são montadas em um carro podem ser facilmente determinados. Mão de obra indireta ou depreciação de máquinas são mais difíceis. Na contabilidade, o conceito de Custeio Baseado em Atividades (ABC) foi desenvolvido para atribuir mais precisamente custos indiretos a produtos e serviços, e a clientes individuais. A motivação do ABC é que os recursos humanos e as máquinas são frequentemente compartilhados por diferentes produtos e serviços, e são usados para atender diferentes clientes. Por exemplo, o depósito da BuildIT aluga máquinas caras, como escavadeiras, para diferentes canteiros de obras. Por um lado, isso envolve custos em termos de horas de trabalho das pessoas que trabalham no depósito. Por outro lado, máquinas como escavadeiras perdem valor ao longo do tempo e requerem manutenção. A ideia do ABC é usar atividades para distribuir os custos indiretos, por exemplo, associados ao depósito.

**Exemplo 10.1**

De acordo com a Figura 1.6 no Capítulo 1, o processo de aluguel da BuildIT contém cinco atividades principais. Observamos as seguintes durações nos logs de eventos para o caso de um aluguel de escavadeira solicitado em 21 de agosto:

- "Enviar solicitação de aluguel de equipamento" é realizada pelo engenheiro do site. Leva 20 minutos para preencher o formulário em papel. A produção de cada formulário em papel custa €1. O engenheiro do site recebe um salário anual de €60.000.
- O escriturário recebe o formulário e seleciona o equipamento adequado e verifica a disponibilidade. Ambas as atividades juntas levam 15 minutos. O escriturário trabalha a uma taxa anual de €40.000.
- O engenheiro de obras revisa a solicitação de aluguel (salário anual de €50.000). Esta revisão leva 10 minutos.
- O escriturário também é responsável por enviar uma confirmação, incluindo um pedido de compra para alugar o equipamento, o que leva 30 minutos.

Para trabalhar com esses números, precisamos fazer certas suposições. Primeiro, na BuildIT, o ano de trabalho real contém 250 dias úteis de 8 horas. Além disso, todos os funcionários recebem seguro de saúde e contribuições para aposentadoria de 20% sobre seus salários. Finalmente, as pessoas estão, em média, 10 dias de licença médica por ano. Levando isso em consideração, podemos calcular o custo de mão-de-obra de cada participante por minuto como salário × 120% / ((250 - 10) × 8 × 60). Isso é para o engenheiro do site €0,63 por minuto, para o escriturário €0,42 por minuto, e para o engenheiro de obras €0,52 por minuto. No total, este caso criou custos de 20 minutos × €0,63 por minuto + (15 + 30) minutos × €0,42 por minuto + 10 minutos × €0,52 por minuto, que somam €36,70.

Agora, considere um caso de um processo de construção de rua. Observamos as seguintes durações dos logs de eventos para essas duas atividades:

- "Preparar a fundação" é realizada por quatro trabalhadores. Levou uma semana em um caso específico de construção. A escavadeira usada custa €100.000. É depreciada em cinco anos e tem custos de manutenção anuais de €5.000. Um trabalhador recebe um salário anual de €35.000.
- "Asfaltar a estrada" é realizada por seis trabalhadores. Levou dois dias neste caso. A máquina de asfalto custa €200.000, também depreciada em cinco anos e tem custos de manutenção anuais de €10.000.

Para este caso, também podemos considerar os custos das máquinas. O custo de mão-de-obra por dia é de €175 por trabalhador. A escavadeira custa €20.000 + 5.000 por ano para depreciação e manutenção, que é €104,17 por dia útil. Para a preparação da fundação, isso totaliza 4 × 5 × €175 + 5 × €104,17 = €4.020,85. Para o asfalto da estrada, os custos são 6 × 2 × €175 + 2 × €208,34 = €2.516,68.

**Exercício 10.8:** Considere que o formulário em papel é impresso pelo engenheiro do site no processo de aluguel, que a impressora custa €300 depreciada em três anos e que uma pilha de 500 folhas de papel custa €10. Por que poderia fazer sentido incluir esses custos no cálculo, por que não?

Um problema inerente ao ABC é o detalhamento dos dados necessários para rastrear a duração das atividades, como alugar equipamentos ou aprovar solicitações de aluguel. Dados de eventos armazenados em sistemas de informação conscientes do processo podem ajudar a fornecer tais dados. Além disso, tecnologias como identificação por radiofrequência (RFID), que ajuda a rastrear objetos físicos com base em pequenos chips RFID anexados a eles, são promissoras para superar o problema de rastreamento do ABC. Alguns sistemas rastreiam apenas a conclusão da atividade. No entanto, o ABC também exige que o início das atividades seja rastreado. Isso pode ser conseguido rastreando timestamps do ponto no tempo em que um recurso começa a trabalhar em uma tarefa específica. O importante aqui é levar em conta o custo de se obter transparência adicional. Há um trade-off, e uma vez que se torna excessivamente caro ganhar mais transparência, é bom não incluir esses custos no cálculo.

### 10.3.3 Medição de Qualidade

A qualidade de um produto criado em um processo muitas vezes não é diretamente visível nos logs de execução. No entanto, uma boa indicação é verificar se há repetições nos logs de execução, pois geralmente ocorrem quando uma tarefa não foi concluída com sucesso. As repetições podem ser encontradas em sequências de tarefas. No Capítulo 7, vimos que o loop de um padrão de retrabalho aumenta o tempo de ciclo de uma tarefa para CT = T / (1 − r) em comparação com T sendo o tempo para executar a tarefa apenas uma vez. A questão agora é como podemos determinar a probabilidade de repetição r a partir de uma série de logs de eventos?

A primeira parte da resposta a essa pergunta pode ser dada reformulando a equação de forma que seja resolvida para r. Ao multiplicar por 1 − r, obtemos CT − r × CT = T. A subtração de CT resulta em −r × CT = T − CT, que pode ser dividida por −CT resultando em

\[ r = 1 - \frac{T}{CT} \]

Tanto CT quanto T podem agora ser determinados usando os dados dos logs de eventos. Considere os cinco casos em que observamos os seguintes tempos de execução para a tarefa a:

1. 5 minutos, 10 minutos.
2. 10 minutos.
3. 20 minutos, 6 minutos, 10 minutos.
4. 5 minutos.
5. 10 minutos, 10 minutos.

O tempo de ciclo CT de a pode agora ser calculado como o tempo médio de execução de a por caso, enquanto o tempo médio de execução T é o tempo médio de execução de a por instanciação. Ambos podem ser determinados com base na soma de todas as execuções de a, que é 86 minutos aqui. Temos cinco casos, de forma que CT = 86/5 = 17,2. No total, a é executada nove vezes, resultando em T = 86/9 = 9,56. Assim, obtemos r = 1 - 9,56/17,2 = 1 - 5/9 = 0,44. Claro, esse cálculo é apenas uma aproximação do valor real para r. Baseia-se na suposição de que a duração de uma tarefa sempre segue a mesma distribuição, não importa se é a primeira, a segunda ou outra iteração.

**Exercício 10.9:** Determine a probabilidade de repetição r para os seguintes tempos de execução da tarefa b:

1. 20 minutos, 10 minutos.
2. 30 minutos.
3. 30 minutos, 5 minutos.
4. 20 minutos.
5. 20 minutos, 5 minutos.
6. 25 minutos.

Explique também por que o valor é enganoso para esses logs.

Em alguns sistemas de informação, pode ser mais fácil rastrear a repetição com base na atribuição de tarefas a recursos. Um exemplo são os sistemas de ticketing que registram qual recurso está trabalhando em um caso. Além disso, os logs desses sistemas oferecem insights sobre repetição. Um processo típico apoiado por sistemas de ticketing é a resolução de incidentes. Por exemplo, um incidente pode ser uma chamada de um cliente que reclama que o sistema de banco online não funciona. Tal incidente é registrado por um participante dedicado, por exemplo, um agente de call center. Em seguida, é encaminhado para uma equipe de suporte de primeiro nível que tenta resolver o problema. Caso o problema se revele muito específico, é encaminhado para uma equipe de suporte de segundo nível com conhecimento especializado no domínio do problema. No melhor dos casos, o problema é resolvido e o cliente é notificado. No caso indesejável, a equipe identifica que o problema está dentro da competência de outra equipe. Isso tem como consequência que o problema é encaminhado de volta para a equipe de primeiro nível. Semelhante à repetição de tarefas, agora vemos que há uma repetição na atribuição do problema à mesma equipe. As informações de log de acordo podem ser usadas para determinar a probabilidade de um problema ser encaminhado de volta.

### 10.3.4 Medição de Flexibilidade

Flexibilidade refere-se ao grau de variação que um processo permite. Essa flexibilidade pode ser discutida em relação aos logs de eventos que o processo produz. Para a empresa proprietária do processo, essa é uma informação importante para comparar o nível desejado de flexibilidade com a flexibilidade real. Pode-se descobrir que o processo é mais flexível do que o exigido do ponto de vista dos negócios. Este é o caso quando a flexibilidade pode ser equiparada à falta de padronização. Frequentemente, o desempenho dos processos sofre quando muitas opções são permitidas. Considere novamente o processo de aluguel de equipamentos na BuildIT. O processo exige que um formulário de solicitação de aluguel de equipamento seja enviado por e-mail. No entanto, alguns engenheiros preferem ligar diretamente para o depósito em vez de preencher o formulário. Como esses engenheiros são frequentemente altamente qualificados, não é fácil para o escriturário recusar essas chamadas. Como resultado, o escriturário preenche o formulário enquanto está ao telefone. Esse procedimento não apenas leva mais tempo, mas também, devido ao ruído no canteiro de obras, aumenta a probabilidade de erros. Na prática, isso significa que o processo de aluguel tem duas opções para enviar uma solicitação: por formulário (o procedimento padrão) e por telefone.

Parcialmente, essa flexibilidade descrita acima pode ser diretamente observada nos logs de eventos. Vimos que o log de workflow de um processo desempenha um papel importante para a descoberta automática de processos. Também pode ser usado para avaliar a flexibilidade do processo. O log de workflow resume o comportamento essencial do processo. Como define cada execução como uma sequência de tarefas, abstrai da distância temporal entre elas. Dessa forma, o log de workflow contém um conjunto de traços que têm uma sequência única. Isso significa que se duas execuções contiverem a mesma sequência de tarefas, então resultam em apenas um traço a ser incluído no log de workflow. Essa abstração das execuções do processo em termos de um log de workflow faz dele um bom ponto de partida para discutir flexibilidade. Assim, o número de execuções distintas DE pode ser definido com base em um log de workflow L como

\[ DE = |L| \]

**Exercício 10.10:** Considere os logs de eventos do processo de atendimento de pedidos na Figura 10.1. Qual é o número de execuções distintas DE?

A questão que surge é se o número de execuções distintas sempre fornece uma boa indicação de flexibilidade. Às vezes, o número de execuções distintas pode fornecer uma quantificação excessivamente alta de flexibilidade. Este pode ser o caso para processos com concorrência. Tais processos podem ser altamente estruturados, mas tendo apenas um pequeno conjunto de tarefas concorrentes resulta em um conjunto rico de sequências de execução potenciais. Considere o modelo de processo construído pelo algoritmo α na Figura 10.7. As tarefas i e h são concorrentes a j e k. De fato, existem seis opções para executá-las:

1. i,h,j,k
2. j,k,i,h
3. i,j,k,h
4. j,i,h,k
5. i,j,h,k
6. j,i,k,h

Embora a ordem não seja estrita, todas elas devem ser executadas. Portanto, pode ser uma boa ideia considerar adicionalmente se uma tarefa é opcional. Se T se refere ao número de tarefas que aparecem no log de workflow, então o conjunto Topt contém aquelas tarefas que são opcionais. Opcionalidade de acordo com o log significa que para uma tarefa específica existe pelo menos um traço no qual ela não ocorre. Para os logs da Figura 10.1, observamos que as tarefas b a f dependem da disponibilidade de matérias-primas. Podemos quantificar o grau de opcionalidade OPT como

\[ OPT = \frac{Topt}{T} \]

Também podemos abordar a questão da flexibilidade a partir da perspectiva do modelo de processo descoberto automaticamente. Isso tem algumas vantagens, já que o grau de opcionalidade não revela quantas decisões precisam ser tomadas. Se considerarmos o modelo de processo construído pelo algoritmo α na Figura 10.7, vemos que um nó de decisão é incluído (divisão XOR). Ele distingue a situação se o produto está em estoque ou não. Esta observação pode ser quantificada como o número de pontos de decisão descobertos (DDP). Por exemplo, o algoritmo α pode ser usado para esse fim.

## 10.4 Verificação de Conformidade

Enquanto a análise de desempenho está preocupada com a medição de indicadores de desempenho, a verificação de conformidade está preocupada com a questão de saber se a execução de um processo segue regras ou restrições predefinidas. Esta questão pode ser respondida inspecionando os logs de eventos. Se uma restrição particular não for válida, falamos de uma violação. A verificação de conformidade está preocupada em identificar essas violações e fazer declarações sobre a extensão delas no total. Essas informações fornecem insights importantes para o proprietário do processo. Aparentemente, quando as regras não são seguidas, deve-se tomar medidas corretivas. As violações podem se relacionar a uma das três perspectivas do processo de fluxo de controle, dados e recursos isoladamente ou em combinação. A seguir, descrevemos como elas podem ser especificadas.

### 10.4.1 Conformidade do Fluxo de Controle

A conformidade do fluxo de controle pode ser estudada de duas maneiras, com base em restrições explícitas ou em um modelo de processo normativo. Ambas podem ser relacionadas entre si, pois muitas restrições podem ser automaticamente derivadas de um modelo de processo. Primeiro, veremos a abordagem assumindo restrições explícitas, depois discutiremos o uso de um modelo de processo normativo.

Focamos em três tipos de restrições relacionadas ao fluxo de controle: obrigatoriedade, exclusividade e ordenação. Todos esses três tipos de restrição definem como duas atividades podem estar relacionadas em um processo. Uma empresa pode querer definir que certas atividades são obrigatórias porque são necessárias do ponto de vista de controle. Considere novamente o caso da BuildIT e o processo de aluguel de equipamentos. Um engenheiro de obras deve revisar a solicitação de aluguel. Esta atividade serve como um controle para garantir que apenas equipamentos apropriados sejam alugados. Esta medida pode ajudar a manter os custos de aluguel em linha. Tais atividades de controle são candidatas prováveis para serem atividades obrigatórias. No nível dos logs, violações de atividades obrigatórias podem ser encontradas procurando por traços nos quais elas são omitidas. A exclusividade pode ser especificada para atividades que se relacionam a uma decisão. Se, por exemplo, uma solicitação de aluguel for rejeitada, a BuildIT deseja garantir que não haja como sobrescrever essa decisão. No nível do log, isso significa que nunca deve ser possível que uma rejeição de solicitação seja seguida por uma aprovação. Esta exclusividade pode ser verificada procurando por traços nos quais ambas as atividades exclusivas aparecem. A ordem das atividades pode ser de importância específica para equilibrar o desempenho do processo. No processo de aluguel de equipamentos, primeiro a disponibilidade do equipamento solicitado é verificada antes que a solicitação seja revisada. Esta restrição de ordem ajuda a aliviar a carga de trabalho do engenheiro de obras que deve revisar a solicitação. Obviamente, é um desperdício de esforço revisar solicitações que não podem ser atendidas porque o equipamento não está disponível. Violações de restrições de ordem podem ser encontradas procurando por traços com as atividades aparecendo na ordem errada.

**Exercício 10.11:** Considere os logs de eventos do processo de atendimento de pedidos na Figura 10.1. Quais atividades podem ser consideradas obrigatórias e exclusivas entre si?

A conformidade do fluxo de controle também pode ser verificada comparando o comportamento observado nos logs com um modelo de processo normativo. A ideia aqui é reproduzir cada traço no log de workflow e registrar em cada etapa se uma atividade foi permitida de acordo com as regras do modelo. Normalmente, isso requer a reprodução dos logs no modelo de processo. Com base nas regras de transição do BPMN, podemos reproduzir o caso a,b,g,i,j,k,l no modelo mostrado na Figura 10.11. No estado inicial, o processo tem um token no evento de início. Uma vez iniciado o caso, este token se move para o arco de saída do evento de início. Este arco leva à atividade a ("Verificar disponibilidade de estoque"), o que significa que o token permite que esta atividade seja executada. O token se move para esta atividade enquanto é executada, e é encaminhado para o arco de saída uma vez que a atividade é concluída. Agora, a divisão XOR é ativada, o que significa que uma decisão deve ser tomada para continuar com b ("Recuperar produto do armazém") ou com f ("Fabricar produto"). Para o caso considerado, continuamos com b. Da mesma forma, podemos continuar. Após g ("Confirmar pedido") ser concluído, chegamos a uma divisão AND. Uma divisão AND consome um token de seu arco de entrada e cria um token em cada um de seus arcos de saída. Como resultado, temos dois tokens posteriormente: um permitindo i ("Enviar produto") e um permitindo j ("Emitir fatura"). Neste estado, podemos prosseguir com i ou j. Essas atividades são concorrentes. Para reproduzir o caso, primeiro precisamos continuar com i e depois com j. Quando ambas as atividades são concluídas, encontramos uma junção AND. Esta consome tokens em seus arcos de entrada e cria um token em seu arco de saída. Finalmente, o token ativa k ("Receber pagamento") e leva a l ("Concluir pedido") e, finalmente, o evento de fim.

Esta reprodução de um caso em um modelo de processo normativo também pode ser automatizada. No entanto, não é trivial e requer uma compreensão do comportamento de um modelo de processo. O algoritmo básico da reprodução de tokens foi aprimorado e vários pacotes de software, como ProM, oferecem suporte para isso. Na prática, a reprodução de tokens é considerada um problema intratável devido ao grande espaço de estados que precisa ser investigado. No entanto, o uso de um modelo de processo normativo tem várias vantagens para a verificação de conformidade. Primeiro, evita a necessidade de especificar cada regra de forma explícita, o que pode ser um trabalho demorado e sujeito a erros. Em segundo lugar, a reprodução de tokens fornece uma medida de quantas vezes um log foi reproduzido com sucesso, o que pode ser usado como uma métrica de conformidade.

### 10.4.2 Conformidade de Dados

Enquanto a conformidade do fluxo de controle está preocupada com a ordem e a presença de atividades, a conformidade de dados está preocupada com a verificação da validade dos dados usados ou produzidos durante a execução do processo. A conformidade de dados pode ser verificada usando regras de validação de dados que definem as condições que devem ser atendidas para que os dados sejam considerados válidos. As regras de validação de dados podem ser especificadas em diferentes níveis de granularidade. Por exemplo, podemos especificar uma regra que define que um campo de dados deve ser preenchido (não nulo) ou que um campo de dados deve conter um valor dentro de um intervalo específico. As regras de validação de dados podem ser verificadas inspecionando os valores dos campos de dados nos logs de eventos. Se um valor de campo não atender a uma regra de validação, isso é considerado uma violação de conformidade de dados.

### 10.4.3 Conformidade de Recursos

A conformidade de recursos está preocupada com a verificação se os recursos alocados para executar as atividades do processo estão qualificados e autorizados para fazê-lo. A conformidade de recursos pode ser verificada usando regras de alocação de recursos que definem as condições que devem ser atendidas para que um recurso seja considerado qualificado e autorizado. As regras de alocação de recursos podem ser especificadas em diferentes níveis de granularidade. Por exemplo, podemos especificar uma regra que define que um recurso deve ter uma determinada certificação ou treinamento para executar uma atividade específica. As regras de alocação de recursos podem ser verificadas inspecionando os atributos dos recursos nos logs de eventos. Se um recurso alocado para uma atividade não atender a uma regra de alocação, isso é considerado uma violação de conformidade de recursos.

## 10.5 Resumo

Neste capítulo, discutimos como os dados de eventos podem ser usados para fornecer insights sobre a execução de processos de negócios. Primeiro, investigamos a estrutura dos logs de eventos, sua relação com os modelos de processo e sua utilidade para monitoramento e controle de processos. Em seguida, discutimos três objetivos principais da análise inteligente de processos: transparência, desempenho e conformidade. Discutimos a descoberta automática de processos como um passo técnico para alcançar a transparência de como o processo é executado na realidade. Depois, estudamos como a análise de logs de eventos pode fornecer insights sobre o desempenho do processo. Finalmente, discutimos como a conformidade entre logs de eventos e um modelo de processo pode ser verificada.

Os logs de eventos fornecem uma base rica para análise inteligente de processos. No entanto, a extração e o uso eficaz desses dados exigem um entendimento detalhado dos sistemas de informação que os produzem, bem como das técnicas de análise disponíveis. A mineração de processos é um campo em rápida evolução, com novas técnicas e ferramentas sendo desenvolvidas continuamente para melhorar a precisão e a utilidade dos insights obtidos a partir dos dados de eventos.

